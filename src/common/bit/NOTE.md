js 中很少直接接触二进制数据，最近在做 base64 相关需求，简单补充了一下基础知识：

## 二进制

**位 (bit)**：位是计算机中最小的数据单位。为二进制，只有 `0` 和 `1` 两个值。

**字节 (Byte)**：字节是计算机中数据存储和处理的基本单位（最小计数单位）。8位为1字节。可以表示最大 `255` 的无符号整数或 `-128 ~ 127` 的整数。

js 中只有一种数字类型：Number，内部使用 64 位浮点数（8 字节）

## 位运算

```
数字 a = 5 (二进制: 0101)
数字 b = 3 (二进制: 0011)

1. & (AND 与运算)
   0101
 & 0011
   ----
   0001 = 1
   规则: 两位都是1，结果才是1

2. | (OR 或运算)
   0101
 | 0011
   ----
   0111 = 7
   规则: 任意一位是1，结果就是1

3. ^ (XOR 异或运算)
   0101
 ^ 0011
   ----
   0110 = 6
   规则: 两位不同，结果是1

4. << (左移)
   5 << 1 = 10
   0101 → 1010
   相当于: 5 × 2 = 10

5. >> (右移)
   5 >> 2 = 1
   0101 → 0001
   相当于: 5 ÷ 2^2 = 1（向下取整）
```

实际应用：

```js
// 权限系统
const PERMISSION = {
  READ:    1 << 0,  // 0001 = 1
  WRITE:   1 << 1,  // 0010 = 2
  EXECUTE: 1 << 2,  // 0100 = 4
  DELETE:  1 << 3   // 1000 = 8
};

let userPermission = 0;
// 添加权限
userPermission |= PERMISSION.READ;
userPermission |= PERMISSION.WRITE;
// 删除权限
userPermission &= ~PERMISSION.WRITE;
// 检查权限
const hasReadPermission = (userPermission & PERMISSION.READ) !== 0;
const hasWritePermission = (userPermission & PERMISSION.WRITE) !== 0;
const hasExecutePermission = (userPermission & PERMISSION.EXECUTE) !== 0;
```

## 字节序（Endianness）

字节序是指多字节数据在内存中的存储顺序。当一个数据类型占用多个字节时（如32位整数占4个字节），这些字节在内存中的排列顺序就有两种方式。

例如数字 `0x12345678`，它有4个字节：`12`、`34`、`56`、`78`。

- **大端序（Big-Endian）**：高位字节存储在低地址处，低位字节存储在高地址处（符合人类阅读习惯）。内存顺序为 `12 34 56 78`。
```
内存地址:  0x00    0x01    0x02    0x03
存储内容:   12      34      56      78
          ↑高位字节              ↑低位字节
```

- **小端序（Little-Endian）**：低位字节存储在低地址处，高位字节存储在高地址处（CPU处理更高效）。内存顺序为 `78 56 34 12`。
```
内存地址:  0x00    0x01    0x02    0x03
存储内容:   78      56      34      12
          ↑低位字节              ↑高位字节
```

x86/x64 架构的 CPU 通常使用小端序，而某些网络协议和文件格式则采用大端序。

```js
const buffer = new ArrayBuffer(16);

const int16View = new Int16Array(buffer);
int16View[0] = 258;
// 258 的二进制表示为 00000001 00000010，占用两个字节，按照小端序存储为 00000010 00000001
// 此时内存布局为：00000010 00000001 ... 00000000
// Int16Array 读取时，按小端序解释：前一个字节作为低8位，后一个字节作为高8位，组合成 00000001 00000010，也即 258
console.log(int16View[0]); // 输出 258

const int8View = new Int8Array(buffer);
// 内存布局为：00000010 00000001 ... 00000000
// 按字节读取时，每次只取一个字节，所以不涉及字节序问题
// 第 0 项是 00000010（即 2），第 1 项是 00000001（即 1）
console.log(int8View[0]); // 输出 2
console.log(int8View[1]); // 输出 1
```

## ArrayBuffer

ArrayBuffer 就是一块原始的、固定大小的二进制数据缓冲区，不能直接操作它的数据，需要通过 TypedArray 或 DataView 来读写。

```js
// 创建 4 字节的内存，4 是字节数，不是元素个数
const buffer = new ArrayBuffer(4);
// byteLength 为字节长度，即字节数，不是位数，也不是元素个数
console.log(buffer.byteLength); // 4
// 创建一个视图，表示 4 字节中的 4 个 8 位有符号整数
const int8View = new Int8Array(buffer);
console.log(int8View.length);     // 4，元素个数
console.log(int8View.byteLength); // 4，字节长度
// 向视图中写入数据
int8View[3] = 900;
// 900 的二进制是 1110000100，而 int8 的每一项为 8 位，故写入后 8 位即 10000100
// 此时buffer 中二进制数据是：00000000 00000000 00000000 10000100，对应的八位有符号整数数组是 [0, 0, 0, -124]
console.log(int8View[2]); // 输出 0
console.log(int8View[3]); // 输出 -124
// 使用另一个视图来读取相同的二进制数据
const int32View = new Int32Array(buffer);
console.log(int32View.length);     // 1，元素个数
console.log(int32View.byteLength); // 4，字节长度
// buffer 中的二进制数据没有变，仍然是：00000000 00000000 00000000 10000100
// JavaScript 使用小端序（Little-Endian），读取 Int32Array 时，低地址是低位字节，高地址是高位字节
// 因此解释为：10000100 00000000 00000000 00000000 对应的三十二位有符号整数数组是 [-2080374784]
console.log(int32View[0]); // 输出 -2080374784
```

TypedArray 也可单独使用，单独使用时，会自动创建一个 ArrayBuffer，可作为数组来使用，

```js
const arr1 = new Uint8Array(2); // 创建一个包含 2 个 8 位无符号整数的数组
console.log(arr1.length); // 2
const arr2 = new Uint8Array([10, 20, 30, 40, 50]); // 通过数组初始化（仅能接收该类型范围内的数值）
console.log(arr2.length); // 5
```

DataView 提供了更灵活的方式来读写 ArrayBuffer 中的数据，可以按需读取不同类型的数据，并且可以**手动控制字节序**。

```js
const buffer = new ArrayBuffer(8);
const view = new DataView(buffer);
// 写入 32 位整数 500，使用大端序（false）
view.setInt32(0, 500, false);
// 500 的二进制是 00000000 00000000 00000001 11110100，占用 4 个字节
// 大端序存储：高位在前，低位在后，即 00000000 00000000 00000001 11110100
// 此时内存布局：00000000 00000000 00000001 11110100 00000000 00000000 00000000 00000000
console.log(view.getInt32(0, false)); // 500，用大端序读取
// 如果用小端序读取同一位置，会按照不同的规则解释字节
// 前8个字节内存布局为：00000000 00000000 00000001 11110100
// 小端序认为地址0是低位，地址3是高位，组合成：11110100 00000001 00000000 00000000，解释为 int32 即 -201261056
console.log(view.getInt32(0, true));  // -201261056

// 在偏移量 3 写入 16 位整数 600，使用小端序（true）
view.setInt16(3, 600, true);
// 600 的二进制是 00000010 01011000，占用 2 个字节
// 小端序存储：低位在前，高位在后，即 01011000 00000010
// 此时内存布局：00000000 00000000 00000001 01011000 00000010 00000000 00000000 00000000
// 注意：偏移量 3 的数据被覆盖了（原来是 11110100），偏移量 4 被写入了新数据（原来是 00000000）
// 再读取偏移量 0 的 32 位整数
console.log(view.getInt32(0, false)); // 344，因为最后一个字节被覆盖（00000000 00000000 00000001 01011000 即 344）
console.log(view.getInt16(3, true));  // 600，读取刚写入的数据
```

## BaseN

BaseN（不管是 Base64 还是 Base32、Base16）都是一种将二进制数据编码为文本字符串的方式。二进制数据本身没有字符集的概念，只是一串字节。而 BaseN 编码将这些字节按特定规则重新分组，并映射到一个预定义的可打印字符集上，从而可以在文本环境中安全传输。

所以，在转 BaseN 之前，首先要将要转换的字符转换为二进制表示，然后按 BaseN 的规则重新分组，最后映射到对应的字符集上，生成最终的 BaseN 编码字符串。

以 Base64 为例，Base64 使用 64 个可打印字符（A-Z、a-z、0-9、+、/）来表示二进制数据。一个 Base64 字符表示 6 位二进制数据（因为 2^6 = 64）。因此，三个字节（3 × 8 = 24 位）可以被拆成四个 Base64 字符（4 × 6 = 24 位）。（每组不足6位时向右补0，结果不足4的倍数时补 `=`）

例：字符 `BKou` 转换为 Base64

原始字符串 `BKou` 的 ASCII 编码是 `66 75 111 117`，对应的二进制为：

```
B        K        o        u
66       75       111      117
01000010 01001011 01101111 01110101
```

重新分组并映射到 Base64 字符集，每6位一组：

```
010000 100100 101101 101111 011101 01
010000 100100 101101 101111 011101 010000
16     36     45     47     29     16
Q      k      t      v      d      Q      =      =
```

因此，`BKou` 编码为 Base64 后是 `QktvdQ==`

## ASCII & Unicode & UTF-N & GB2312

众所周知，计算机只能处理二进制数据，而我们日常使用的文本是由字符组成的。那么就一定要有一种机制，将字符映射为二进制数据。

ASCII 就是一种编码标准，它使用 7 位二进制数来表示 128 个字符（包括英文字母、数字、标点符号和一些控制字符），通常使用 8 位即 1 个字节来存储一个 ASCII 字符，最高位为 0。但是 ASCII 只能表示有限的字符集，无法满足全球多种语言的需求。

为了解决多语言问题，Unicode 标准被引入，它给世界上所有字符分配了唯一的编号（码点），范围从 `U+0000` 到 `U+10FFFF`。Unicode 范围包含约 110 万个码点位置，目前已定义了超过 15 万个字符，可以表示几乎所有已知的文字系统。

```
字符  Unicode码点  十六进制表示
A     U+0041       0x0042
中    U+4E2D       0x4E2D
😊    U+1F60A      0x1F60A
```

注意：Unicode 只是一个字符集，定义了"字符→编号"的映射，但没有规定如何在计算机中存储这些编号。

UTF-N 则是一系列用于将 Unicode 码点编码为字节序列的编码方式，常见的有 UTF-8、UTF-16 和 UTF-32。以 UTF-8 为例，编码规则如下：

```
Unicode 范围             UTF-8 编码格式
U+0000  ~ U+007F         0xxxxxxx (1字节，兼容ASCII)
U+0080  ~ U+07FF         110xxxxx 10xxxxxx (2字节)
U+0800  ~ U+FFFF         1110xxxx 10xxxxxx 10xxxxxx (3字节)
U+10000 ~ U+10FFFF       11110xxx 10xxxxxx 10xxxxxx 10xxxxxx (4字节)
```

所以说，从“字符编码方案”的角度看，UTF-8 和 ASCII 属于同一类。

而 GB2312 是中国国家标准，在定位上相当于 Unicode + UTF-8 的合体（既定义字符集，也定义编码方案），专用于简体中文字符编码。它使用双字节编码，中文效率高于 UTF-8 的三字节。但由于其字符集有限，主要用于早期的中文计算机系统，现已逐渐被更全面的 Unicode 和 UTF-8 所取代。
